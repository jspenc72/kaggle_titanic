{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import math as m\n",
    "from statsmodels import robust\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Currently getting warning : \n",
    "# label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. \n",
    "# Returning False, but in future this will result in an error. Use 'array.size > 0' to check that \n",
    "# an array is not empty.\n",
    "#\n",
    "# ... which was apparently fixed pre sklearn 0.19.1 but didn't make it into the build, suppressing for now...\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('./data/train.csv', header=0)\n",
    "test_df = pd.read_csv('./data/test.csv', header=0)\n",
    "\n",
    "CV_FOLDS = 10\n",
    "SCORER = make_scorer(accuracy_score)\n",
    "UNKNOWN_VALUE = \"XX\"\n",
    "LABEL_COL = 'Survived'\n",
    "ID_COL = 'PassengerId'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prefix(input_string):\n",
    "    first_period_index = str.index(input_string, \".\")\n",
    "    prev_space_index = input_string.rfind(\" \", 0, first_period_index-1)\n",
    "    return input_string[prev_space_index+1:first_period_index+1]\n",
    "\n",
    "def get_ticket_code(ticket):\n",
    "    if (ticket != \"\" and not pd.isnull(ticket)):        \n",
    "        ticket_split = ticket.split(\" \")\n",
    "        if len(ticket_split) == 1:\n",
    "            return UNKNOWN_VALUE\n",
    "        else:\n",
    "            return ticket_split[0].strip().replace(\".\",\"\").replace(\"/\",\"\").lower()\n",
    "    else:\n",
    "        return UNKNOWN_VALUE\n",
    "\n",
    "# cols which will be used as features (directly (Fare) or indirectly (Name -> prefix))\n",
    "cols_to_use = ['Fare', 'SibSp', 'Parch', 'Pclass', 'Sex', 'Embarked', 'Name', 'Ticket']\n",
    "\n",
    "# use test and train to ensure we see all possible values\n",
    "train_and_test_X = train_df.append(test_df)\n",
    "train_and_test_X = train_and_test_X.reset_index()\n",
    "\n",
    "# couple items are documented as wrong, why not fix it\n",
    "train_and_test_X.SibSp[train_and_test_X.PassengerId==280] = 0\n",
    "train_and_test_X.Parch[train_and_test_X.PassengerId==280] = 2\n",
    "train_and_test_X.SibSp[train_and_test_X.PassengerId==1284] = 1\n",
    "train_and_test_X.Parch[train_and_test_X.PassengerId==1284] = 1\n",
    "\n",
    "train_and_test_X = train_and_test_X[cols_to_use]\n",
    "\n",
    "# new features\n",
    "train_and_test_X['Prefix'] = train_and_test_X['Name'].apply(get_prefix)\n",
    "train_and_test_X['TicketCode'] = train_and_test_X['Ticket'].apply(get_ticket_code)\n",
    "\n",
    "# Overall Family Size\n",
    "train_and_test_X['ParchSibSp'] = train_and_test_X[['Parch', 'SibSp']].apply(lambda r : r[0] + r[1], axis=1)\n",
    "\n",
    "# These two lists control what raw features will be used, and how they will be treated\n",
    "numeric_cols = ['Fare', 'ParchSibSp', 'Pclass']\n",
    "categorical_cols = ['Sex', 'Prefix', 'Embarked', 'TicketCode']\n",
    "\n",
    "#subset to cols we'll use as features\n",
    "train_and_test_X = train_and_test_X[numeric_cols + categorical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Impute a particular column grouping by another column\n",
    "# Fill in with median if numeric, most common value otherwise\n",
    "#\n",
    "class ContextualImputer(TransformerMixin):\n",
    "    \n",
    "    # constructor params:\n",
    "    #  col : col to impute\n",
    "    #  byCol : column within which to impute 'col'\n",
    "    def __init__(self, col, byCol):\n",
    "        self.col = col\n",
    "        self.byCol = byCol\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.bycolumn_unique_values = np.unique(X[self.byCol].values)\n",
    "        \n",
    "        if X[self.col].dtype != np.dtype('O'):\n",
    "            bycolumn_fill_values = [np.median((X[pd.notnull(X[self.col]) & (X[self.byCol] == bycolumn_unique_value2)][self.col])) \n",
    "                                  for bycolumn_unique_value2 in self.bycolumn_unique_values]\n",
    "        else:\n",
    "            bycolumn_fill_values = [X[pd.notnull(X[self.col]) & (X[self.byCol] == bycolumn_unique_value)][self.col].value_counts().index[0]  \n",
    "                                  for bycolumn_unique_value in self.bycolumn_unique_values]\n",
    "\n",
    "        bycolumn_values_fill_zipped = np.column_stack((self.bycolumn_unique_values, bycolumn_fill_values))\n",
    "        self.bycolumn_values_fill_zipped_dict = dict(bycolumn_values_fill_zipped)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def fill(row):\n",
    "            if pd.isnull(row[self.col]):\n",
    "                return self.bycolumn_values_fill_zipped_dict[row[self.byCol]]\n",
    "            else:\n",
    "                return row[self.col]\n",
    "\n",
    "        X[self.col] = X[self.col].fillna(X.apply(fill, axis=1))\n",
    "        return X\n",
    "\n",
    "#\n",
    "# Typical imputer found around the web.  Most common value for categoricals,\n",
    "# median for numerics.  \n",
    "#\n",
    "# We'll use it for categoricals, and above customer imputer for numerics\n",
    "#\n",
    "class CategImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        If the Series is of dtype Object, then impute with the most frequent object.\n",
    "        If the Series is not of dtype Object, then impute with the mean.  \n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "                               if X[c].dtype == np.dtype('O')\n",
    "                               else X[c].median() for c in X], index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "#\n",
    "# Imputation of missing vals\n",
    "#\n",
    "# custom imputation for Fare by class, was useful for age too (by prefix for example),\n",
    "# but I ended up not using age\n",
    "all_X_imputed = ContextualImputer('Fare', 'Pclass').fit_transform(train_and_test_X)\n",
    "\n",
    "# Only two missing values and other features most resembled the 'C' group\n",
    "train_and_test_X['Embarked'] = np.where(train_and_test_X['Embarked'].isnull(), 'C', train_and_test_X['Embarked'])\n",
    "\n",
    "# most common value for remaining categoricals\n",
    "all_X_imputed = CategImputer().fit_transform(all_X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X_encoded = all_X_imputed\n",
    "\n",
    "# Onehot encoding of cateogricals using get_dummies\n",
    "# Although Pclass is a number, it's really a code and we'll treat it as such\n",
    "for categorical_col in ['Pclass', 'Embarked']:\n",
    "    all_X_encoded = pd.concat([pd.get_dummies(all_X_imputed[categorical_col], \n",
    "                                              prefix=categorical_col, drop_first=True), \n",
    "                               all_X_encoded], axis=1)\n",
    "\n",
    "# can't leave categoricals numbers, Sex is binary so just map to 0/1\n",
    "all_X_encoded['Sex'] = all_X_encoded['Sex'].map({'male':0, 'female': 1})\n",
    "\n",
    "#\n",
    "# Bin some features by survival likelihood\n",
    "#\n",
    "\n",
    "all_X_encoded[\"ParchSibSp_0\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if s == 0 else 0)\n",
    "all_X_encoded[\"ParchSibSp_12\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if (s == 1 or s == 2) else 0)\n",
    "all_X_encoded[\"ParchSibSp_3\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if (s == 3) else 0)\n",
    "all_X_encoded[\"ParchSibSp_gt3\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if s > 3 else 0)\n",
    "\n",
    "# Prefix.  \n",
    "# Leaving out Master since it actually made a diff for XGB. \n",
    "# Assuming this is due to its redundancy with Male/Single(?)\n",
    "high_prob = ['Mme.', 'Ms.', 'Lady.', 'Sir.', 'Mlle.', 'Countess']\n",
    "medhigh_prob = ['Mrs.', 'Miss.']\n",
    "med_prob = ['Dr.', 'Major.', 'Col.', 'Mrs.']\n",
    "low_prob = ['Mr.']\n",
    "very_low_prob = ['Capt.', 'Don.', 'Rev.', 'Jonkheer.']\n",
    "known_probs = high_prob + med_prob + low_prob + very_low_prob\n",
    "all_X_encoded[\"Prefix_high\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in high_prob else 0)\n",
    "all_X_encoded[\"Prefix_medhigh_prob\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in medhigh_prob else 0)\n",
    "all_X_encoded[\"Prefix_med\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in med_prob else 0)\n",
    "all_X_encoded[\"Prefix_low\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in low_prob else 0)\n",
    "all_X_encoded[\"Prefix_very_low\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in very_low_prob else 0)\n",
    "\n",
    "# TicketCode\n",
    "high_prob = ['swpp', 'sc']\n",
    "med_high_prob = ['pc', 'pp', 'fcc', 'scah']\n",
    "med_prob = ['stono2', 'XX', 'ca', 'scparis', 'stono', 'c', 'ppp']\n",
    "low_prob = ['a5', 'soc', 'wc', 'sotonoq', 'wep']\n",
    "very_low_prob = ['sca4', 'a4', 'sp', 'spo', 'fa', 'scow', 'as', 'sopp', 'fc', 'sotono2', 'casoton']\n",
    "known_probs = high_prob + med_high_prob + med_prob + low_prob + very_low_prob \n",
    "all_X_encoded[\"TicketCode_high\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in high_prob else 0)\n",
    "all_X_encoded[\"TicketCode_medhigh\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in med_high_prob else 0)\n",
    "all_X_encoded[\"TicketCode_med\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in med_prob else 0)\n",
    "all_X_encoded[\"TicketCode_low\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in low_prob else 0)\n",
    "all_X_encoded[\"TicketCode_very_low\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in very_low_prob else 0)\n",
    "\n",
    "all_X_final = all_X_encoded\n",
    "\n",
    "all_X_final.drop(\"Prefix\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"TicketCode\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"Embarked\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"Pclass\", inplace=True, axis=1)\n",
    "\n",
    "test_X = all_X_final[all_X_final.shape[0]-test_df.shape[0]::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV : best estimator accuracy : 0.8451178451178452\n",
      "CV : best estimator std : 0.02811520868332541\n",
      "CV : best estimator range (.95) : 0.7888874277511944 - 0.901348262484496\n",
      "CV : best estimator range (.99) : 0.760772219067869 - 0.9294634711678214\n",
      "CV : best estimator params : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.75, 'reg_lambda': 0.75}\n",
      "CV : xgbc_best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.75, reg_lambda=0.75, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "XGBoost : CV elapsed time : 0.5577900409698486\n",
      "Last run time : 2018-04-16 16:17:32.884099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "   'learning_rate': [.1],\n",
    "# 'learning_rate': [.01, 0.1, .2, .25, .5, .75, 1], \n",
    "     'max_depth': [3],\n",
    "#'max_depth': [2, 3, 4, 5, 6],\n",
    "#        'min_child_weight': [1],\n",
    "#'min_child_weight': [1, 2, 3, 4, 5],\n",
    "\n",
    "'n_estimators': [100],\n",
    "#'n_estimators': [75, 100, 125, 150, 200, 225],\n",
    "#    'subsample' : [.9],\n",
    "#'subsample' : [.6, .7, .8, .9, 1.0],\n",
    "#       'colsample_bytree' : [.6],\n",
    "#'colsample_bytree' : [.6, .7, .8, .9, 1.0],\n",
    "#      'gamma': [0.1],\n",
    "#'gamma': [0, .01, .1, .2, .3, .4],\n",
    "\n",
    "       'reg_alpha':[.75],\n",
    "#'reg_alpha':[0.0, .1, .25, .5, .75, 1],\n",
    "       'reg_lambda':[.75]\n",
    "#'reg_lambda':[0.0, .1, .25, .5, .75, 1]\n",
    "    }\n",
    "\n",
    "grid_obj = GridSearchCV(xgb.XGBClassifier(), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS, \n",
    "                        verbose=1, \n",
    "                        n_jobs=4)\n",
    "\n",
    "xgbc_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"CV : best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"CV : xgbc_best : {0}\".format(xgbc_best))\n",
    "\n",
    "xgb_final_predictions = xgbc_best.predict(test_X)\n",
    "xgb_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': xgb_final_predictions})\n",
    "xgb_final_predictions_df.to_csv(\"submission_xgb.csv\", index=False)\n",
    "print(\"XGBoost : CV elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator accuracy : 0.8316498316498316\n",
      "CV : best estimator std : 0.025699644490569704\n",
      "CV : best estimator range (.95) : 0.7802505426686922 - 0.8830491206309711\n",
      "CV : best estimator range (.99) : 0.7545508981781225 - 0.9087487651215408\n",
      "best estimator params : {'max_depth': 4, 'max_features': 5, 'n_estimators': 100}\n",
      "etc_best : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=4, max_features=5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "elapsed time : 0.815263032913208\n",
      "Last run time : 2018-04-16 16:17:35.312809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "    \"max_depth\": [4],\n",
    "#    \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "    \"max_features\": [5],\n",
    "#    \"max_features\": [3, 4, 5, 6, 7, 8, 9, 10, 12],\n",
    "\n",
    "#      \"min_samples_leaf\": [1],\n",
    "#\"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#      \"min_samples_split\": [2],\n",
    "#\"min_samples_split\": [2, 3, 4, 5],\n",
    "\n",
    "# #   \"min_weight_fraction_leaf\": [.01],\n",
    "#\"min_weight_fraction_leaf\": [0, .01, .1, .2],\n",
    "       \"n_estimators\" :[100],\n",
    "#\"n_estimators\" :[25, 50, 75, 100, 125, 150, 200, 250, 300],\n",
    "    }\n",
    "\n",
    "grid_obj = GridSearchCV(ExtraTreesClassifier(random_state=0), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS, \n",
    "                        verbose=1,\n",
    "                       n_jobs=4)\n",
    "\n",
    "etc_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"etc_best : {0}\".format(etc_best))\n",
    "\n",
    "etc_final_predictions = etc_best.predict(test_X)\n",
    "etc_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': etc_final_predictions})\n",
    "etc_final_predictions_df.to_csv(\"submission_etc.csv\", index=False)\n",
    "print(\"elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "best estimator accuracy : 0.8327721661054994\n",
      "CV : best estimator std : 0.025488819380169093\n",
      "CV : best estimator range (.95) : 0.7817945273451612 - 0.8837498048658377\n",
      "CV : best estimator range (.99) : 0.7563057079649922 - 0.9092386242460067\n",
      "best estimator params : {'C': 3, 'max_iter': 10, 'penalty': 'l1'}\n",
      "lr_best : LogisticRegression(C=3, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "elapsed time : 0.17826414108276367\n",
      "Last run time : 2018-04-16 16:17:36.263247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "    \"max_iter\" : [10],\n",
    "#    \"max_iter\" : [5, 10, 25, 50, 100, 200],\n",
    "    \"penalty\" : ['l1'],\n",
    "#    \"penalty\" : ['l1', 'l2'],\n",
    "    \"C\" : [3],\n",
    "#    \"C\" : [.01, .1, .5, 1, 2, 3, 4, 5, 10],\n",
    "#    \"class_weight\" : [\"balanced\", None]\n",
    "}\n",
    "\n",
    "# Run the grid search over cv split data\n",
    "grid_obj = GridSearchCV(LogisticRegression(), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS, \n",
    "                        verbose=1,\n",
    "                        n_jobs=2)\n",
    "\n",
    "lr_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"lr_best : {0}\".format(lr_best))\n",
    "\n",
    "\n",
    "lr_final_predictions = lr_best.predict(test_X)\n",
    "lr_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': lr_final_predictions})\n",
    "lr_final_predictions_df.to_csv(\"submission_lr.csv\", index=False)\n",
    "print(\"elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of  10 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator accuracy : 0.8002244668911336\n",
      "CV : best estimator std : 0.018644291935906422\n",
      "CV : best estimator range (.95) : 0.7629358830193207 - 0.8375130507629465\n",
      "CV : best estimator range (.99) : 0.7442915910834144 - 0.8561573426988528\n",
      "CV : best estimator params : {'C': 4, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': 10000}\n",
      "CV : svc_best : SVC(C=4, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "elapsed time : 0.9545049667358398\n",
      "Last run time : 2018-04-16 16:17:38.400520\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "    'kernel': ['rbf'], \n",
    "#    'kernel': ['linear', 'rbf', 'poly'], \n",
    "#    \"degree\" : [2],\n",
    "#    \"degree\" : [2, 3, 4],\n",
    "    # this ran seemingly forever without max_iter\n",
    "    'max_iter' : [10000],\n",
    "   'gamma': [.01],\n",
    "#  'gamma': [ 0, .0001, 0.001, 0.01, 0.1, .2, .3],\n",
    "   'C': [4],\n",
    "#  'C': [.1, .5, 1, 2, 3, 4, 10, 20],\n",
    "}\n",
    "\n",
    "grid_obj = GridSearchCV(SVC(probability=True), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS, \n",
    "                        verbose=1,\n",
    "                        n_jobs=4)\n",
    "svc_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"CV : svc_best : {0}\".format(svc_best))\n",
    "\n",
    "\n",
    "svc_final_predictions = svc_best.predict(test_X)\n",
    "svc_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': svc_final_predictions})\n",
    "svc_final_predictions_df.to_csv(\"submission_svc.csv\", index=False)\n",
    "print(\"elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "best estimator accuracy : 0.8226711560044894\n",
      "CV : best estimator std : 0.027583909030662858\n",
      "CV : best estimator range (.95) : 0.7675033379431636 - 0.8778389740658151\n",
      "CV : best estimator range (.99) : 0.7399194289125008 - 0.9054228830964779\n",
      "CV : best estimator params : {'algorithm': 'SAMME.R', 'base_estimator__criterion': 'entropy', 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 10}\n",
      "CV : ada_best : AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=0.1, n_estimators=10, random_state=None)\n",
      "elapsed time : 0.31580400466918945\n",
      "Last run time : 2018-04-16 16:17:38.849622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "    \"base_estimator__criterion\" : [\"entropy\"],\n",
    "#    \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"base_estimator__splitter\" :   [\"best\"],\n",
    "#    \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "    \"n_estimators\": [10],\n",
    "#    \"n_estimators\": [1, 2, 3, 4, 5, 10, 20],\n",
    "    \"learning_rate\":  [.1],\n",
    "#    \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, .2],\n",
    "    \"algorithm\" : ['SAMME.R']\n",
    "#    \"algorithm\" : ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "grid_obj = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier()), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS,\n",
    "                        verbose=1,\n",
    "                        n_jobs=4)\n",
    "\n",
    "ada_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"CV : ada_best : {0}\".format(ada_best))\n",
    "\n",
    "\n",
    "ada_final_predictions = ada_best.predict(test_X)\n",
    "ada_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': ada_final_predictions})\n",
    "ada_final_predictions_df.to_csv(\"submission_ada.csv\", index=False)\n",
    "print(\"elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator accuracy : 0.8395061728395061\n",
      "CV : best estimator std : 0.030241409731174177\n",
      "CV : best estimator range (.95) : 0.7790233533771578 - 0.8999889923018545\n",
      "CV : best estimator range (.99) : 0.7487819436459836 - 0.9302304020330286\n",
      "CV : best estimator params : {'max_depth': 5, 'max_features': 15, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "CV : rf_best : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=15, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "elapsed time : 0.5822420120239258\n",
      "Last run time : 2018-04-16 16:17:40.122693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "start_time = time.time()\n",
    "\n",
    "parameters = {\n",
    "    \"max_depth\": [5],\n",
    "#    \"max_depth\": [2, 3, 4, 5, 6, 10, 15, 20],\n",
    "    \"max_features\" : [15],\n",
    "#    \"max_features\" : [2, 3, 4, 5, 6, 10, 15],\n",
    "    \"min_samples_split\" : [10],\n",
    "#   \"min_samples_split\" : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"n_estimators\": [50],\n",
    "#   \"n_estimators\": [5, 10, 15, 25, 50, 100],\n",
    "}\n",
    "\n",
    "grid_obj = GridSearchCV(RandomForestClassifier(), \n",
    "                        parameters, \n",
    "                        scoring=SCORER, \n",
    "                        cv=CV_FOLDS,\n",
    "                        verbose=1,\n",
    "                        n_jobs=4)\n",
    "rf_best = grid_obj.fit(all_X_final[0:train_df.shape[0]], train_df[LABEL_COL]).best_estimator_\n",
    "print(\"best estimator accuracy : {0}\".format(grid_obj.best_score_))\n",
    "print(\"CV : best estimator std : {0}\".format(grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.95) : {0} - {1}\".format(grid_obj.best_score_ - 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 2*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator range (.99) : {0} - {1}\".format(grid_obj.best_score_ - 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_], grid_obj.best_score_ + 3*grid_obj.cv_results_['std_test_score'][grid_obj.best_index_]))\n",
    "print(\"CV : best estimator params : {0}\".format(grid_obj.best_params_))\n",
    "print(\"CV : rf_best : {0}\".format(rf_best))\n",
    "\n",
    "\n",
    "rf_final_predictions = rf_best.predict(test_X)\n",
    "rf_final_predictions_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': rf_final_predictions})\n",
    "rf_final_predictions_df.to_csv(\"submission_rf.csv\", index=False)\n",
    "print(\"elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier : estimators : ['xgbc_best', 'etc_best', 'ada_best', 'svc_best', 'rf_best']\n",
      "\n",
      "elapsed time : 0.3106560707092285\n",
      "Last run time : 2018-04-16 16:17:40.879782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('xgbc_best', xgbc_best))\n",
    "estimators.append(('etc_best', etc_best))\n",
    "#estimators.append(('lr_best', lr_best)) # this dropped accuracy\n",
    "estimators.append(('ada_best', ada_best))\n",
    "estimators.append(('svc_best', svc_best))\n",
    "estimators.append(('rf_best', rf_best))\n",
    "\n",
    "vc = VotingClassifier(estimators, voting='soft', n_jobs=4)\n",
    "vc.fit(X=all_X_final[0:train_df.shape[0]], y=train_df[LABEL_COL])\n",
    "final_predictions = vc.predict(test_X)\n",
    "final_predictions_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], LABEL_COL: final_predictions })\n",
    "final_predictions_df.to_csv(\"submission_vc.csv\", index=False)\n",
    "print(\"VotingClassifier : estimators : {0}\".format([estimator[0] for estimator in estimators]))\n",
    "print(\"\\nelapsed time : {0}\".format(time.time() - start_time, [estimator[0] for estimator in estimators]))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train shape : (891, 12)\n",
    "# with at least one missing value : (708, 12)\n",
    "# test shape : (418, 11)\n",
    "# with at least one missing value : (331, 11)\n",
    "# train_df[train_df.isnull().any(axis=1)]\n",
    "\n",
    "#\n",
    "# null/missing/nan values\n",
    "#\n",
    "# null_col_counts = []\n",
    "# all_X = train_df[cols_to_use].append(test_df[cols_to_use])\n",
    "# for col in all_X.columns:\n",
    "#     null_col_counts.append((col,train_and_test_X[pd.isnull(all_X[col])].shape[0]))\n",
    "# null_col_counts\n",
    "# [('Age', 263),\n",
    "#  ('Fare', 1),\n",
    "#  ('SibSp', 0),\n",
    "#  ('Parch', 0),\n",
    "#  ('Pclass', 0),\n",
    "#  ('Sex', 0),\n",
    "#  ('Embarked', 2),\n",
    "#  ('Name', 0),\n",
    "#  ('Cabin', 1014)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#corr = pd.concat([train_and_test_X, train_df[LABEL_COL].T], axis=1).corr()\n",
    "\n",
    "# corr = pd.concat([all_X_final, train_df[LABEL_COL].T], axis=1).corr()\n",
    "\n",
    "# # save to csv\n",
    "# corr.to_csv(\"titanic_corr.csv\")\n",
    "\n",
    "# #\n",
    "# # plot correlations in heatmap\n",
    "# #\n",
    "# # Generate a mask for the upper triangle\n",
    "# mask = np.zeros_like(corr, dtype=np.bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# # Generate a custom diverging colormap\n",
    "# sns.set(style=\"white\")\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# fig, ax = plt.subplots(figsize=(16,16)) \n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, \n",
    "#             cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary stats of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_and_test_X[0:train_df.shape[0]][numeric_cols].describe()\n",
    "\n",
    "# all_X_final[0:train_df.shape[0]][['Fare', 'ParchSibSp']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import scipy.stats as stats\n",
    "# import seaborn as sns\n",
    "# import matplotlib.mlab as mlab\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# per_row = 3\n",
    "# fig, axs = plt.subplots(figsize=(20, 5), ncols=per_row) \n",
    "\n",
    "# axs[0].set_title(LABEL_COL)\n",
    "# axs[0].hist(train_df[LABEL_COL])\n",
    "# plot_count = 1\n",
    "\n",
    "# for col in all_X_final.columns:\n",
    "#     row_item_count = plot_count % per_row\n",
    "    \n",
    "#     if (row_item_count == 0):\n",
    "#         fig, axs = plt.subplots(figsize=(20, 5), ncols=per_row) \n",
    "        \n",
    "#     axs[row_item_count].set_title(col)\n",
    "\n",
    "#     if col in numeric_cols:\n",
    "#         sns.distplot(all_X_final[col], ax=axs[row_item_count])\n",
    "#     else:\n",
    "#         sns.distplot(all_X_final[col], ax=axs[row_item_count], kde=False)\n",
    "    \n",
    "#     plot_count = plot_count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribs by label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # use imputed since it still has original columns\n",
    "# all_X_final_train = pd.concat([all_X_imputed[0:train_df.shape[0]], train_df[LABEL_COL]], axis=1)\n",
    "\n",
    "# # have to melt the dataset and use the variable name as the 'row' var in the FacetGrid\n",
    "# # there will be a different graph pair per variable and each pair is split on the 'col' var (the label)\n",
    "# all_X_final_train_melted = all_X_final_train.melt(id_vars=[LABEL_COL], value_vars=numeric_cols)\n",
    "\n",
    "# g = sns.FacetGrid(all_X_final_train_melted, \n",
    "#                   col='Survived', \n",
    "#                   hue='Survived', \n",
    "#                   row='variable', \n",
    "#                   sharex=False, \n",
    "#                   sharey=False, \n",
    "#                   margin_titles=True,\n",
    "#                   size = 5)\n",
    "# g = g.map(sns.distplot, 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = sns.factorplot(x=\"Prefix\", data=all_X_imputed, kind=\"count\", size = 12)\n",
    "#g = sns.factorplot(x=\"SibSp\", data=all_X_imputed, kind=\"count\", size = 5)\n",
    "#g = sns.factorplot(x=\"TicketCode\", data=all_X_imputed, kind=\"count\", size = 15)\n",
    "#g = sns.factorplot(x=\"CabinCount\", data=all_X_imputed, kind=\"count\", size = 6)\n",
    "#g = sns.factorplot(x=\"CabinCode\", data=all_X_imputed, kind=\"count\", size = 6)\n",
    "#g = sns.factorplot(x=\"Embarked\", data=all_X_imputed, kind=\"count\", size = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Categoricals distrib by label, shows percentage with label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_with_label = pd.concat([train_and_test_X[0:train_df.shape[0]], train_df[LABEL_COL]], axis=1)\n",
    "\n",
    "#g = sns.factorplot(x=\"Embarked\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "#g = sns.factorplot(x=\"CabinCode\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 12)\n",
    "#g = sns.factorplot(x=\"TicketCode\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 16)\n",
    "#g = sns.factorplot(x=\"Prefix\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 12)\n",
    "#g = sns.factorplot(x=\"CabinCount\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "#g = sns.factorplot(x=\"Parch\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "#g = sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "#g = sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "#g = sns.factorplot(x=\"ParchSibSp\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 6)\n",
    "\n",
    "# Not categoricals, These are messy and take a minute or so, but worth looking at\n",
    "#g = sns.factorplot(x=\"Age\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 30)\n",
    "#g = sns.factorplot(x=\"Fare\", y=\"Survived\", data=train_with_label, kind=\"bar\", size = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
