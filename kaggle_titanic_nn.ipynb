{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import math as m\n",
    "from statsmodels import robust\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import scipy.special\n",
    "\n",
    "import multiprocessing as mp\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Currently getting warning : \n",
    "# label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. \n",
    "# Returning False, but in future this will result in an error. Use 'array.size > 0' to check that \n",
    "# an array is not empty.\n",
    "#\n",
    "# ... which was apparently fixed pre sklearn 0.19.1 but didn't make it into the build, suppressing for now...\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('./data/train.csv', header=0)\n",
    "test_df = pd.read_csv('./data/test.csv', header=0)\n",
    "\n",
    "LABEL_COL = 'Survived'\n",
    "CV_FOLDS = 10\n",
    "SCORER = make_scorer(accuracy_score)\n",
    "UNKNOWN_VALUE = \"XX\"\n",
    "PERC_SURVIVED = train_df[train_df[LABEL_COL] == 1].shape[0]/train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prefix(input_string):\n",
    "    first_period_index = str.index(input_string, \".\")\n",
    "    prev_space_index = input_string.rfind(\" \", 0, first_period_index-1)\n",
    "    return input_string[prev_space_index+1:first_period_index+1]\n",
    "\n",
    "def get_cabin_code(cabins_string):\n",
    "    if (cabins_string != \"\" and not pd.isnull(cabins_string)):        \n",
    "        return cabins_string.split(\" \")[0][0]\n",
    "    else:\n",
    "        return UNKNOWN_VALUE\n",
    "\n",
    "def get_cabin_count(cabins_string):\n",
    "    if (cabins_string != \"\" and not pd.isnull(cabins_string)):        \n",
    "        return len(cabins_string.split(\" \")[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ticket_code(ticket):\n",
    "    if (ticket != \"\" and not pd.isnull(ticket)):        \n",
    "        ticket_split = ticket.split(\" \")\n",
    "        if len(ticket_split) == 1:\n",
    "            return UNKNOWN_VALUE\n",
    "        else:\n",
    "            return ticket_split[0].strip().replace(\".\",\"\").replace(\"/\",\"\").lower()\n",
    "    else:\n",
    "        return UNKNOWN_VALUE\n",
    "    \n",
    "def get_lastname(input_string):\n",
    "    return input_string[0:str.index(input_string, \",\")]\n",
    "\n",
    "# Cols which will be used as features (directly (Fare) or indirectly (Name -> prefix))\n",
    "cols_to_use = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass', 'Sex', 'Embarked', 'Name', 'Cabin', 'Ticket']\n",
    "\n",
    "# use test and train to ensure we see all possible values\n",
    "train_and_test_X = train_df.append(test_df)\n",
    "train_and_test_X = train_and_test_X.reset_index()\n",
    "\n",
    "# Data is documented as wrong for a couple passengers, why not fix it\n",
    "train_and_test_X.SibSp[train_and_test_X.PassengerId==280] = 0\n",
    "train_and_test_X.Parch[train_and_test_X.PassengerId==280] = 2\n",
    "train_and_test_X.SibSp[train_and_test_X.PassengerId==1284] = 1\n",
    "train_and_test_X.Parch[train_and_test_X.PassengerId==1284] = 1\n",
    "\n",
    "# subset\n",
    "train_and_test_X = train_and_test_X[cols_to_use]\n",
    "\n",
    "# new features\n",
    "train_and_test_X['Prefix'] = train_and_test_X['Name'].apply(get_prefix)\n",
    "train_and_test_X['CabinCode'] = train_and_test_X['Cabin'].apply(get_cabin_code)\n",
    "train_and_test_X['CabinCount'] = train_and_test_X['Cabin'].apply(get_cabin_count)\n",
    "train_and_test_X['TicketCode'] = train_and_test_X['Ticket'].apply(get_ticket_code)\n",
    "\n",
    "# Overall Family Size, count the members plus 1 for the person in question\n",
    "train_and_test_X['ParchSibSp'] = train_and_test_X[['Parch', 'SibSp']].apply(lambda r : r[0] + r[1] + 1, axis=1)\n",
    "\n",
    "train_and_test_X['LastName'] = train_and_test_X['Name'].apply(get_lastname)\n",
    "\n",
    "# This field forms sort of a key defining what group a person is travelling with\n",
    "train_and_test_X['LastName_ParchSibSp'] = train_and_test_X[['LastName', 'ParchSibSp']].apply(lambda r : \"%s_%s\" % (r[0], r[1]), axis=1)\n",
    "train_X = train_and_test_X[0:train_df.shape[0]]\n",
    "train_X_Y = pd.concat([train_X, train_df[LABEL_COL]], axis=1)\n",
    "LastName_ParchSibSp_counts = train_X_Y.groupby(['LastName_ParchSibSp']).size().reset_index(name='LastName_ParchSibSp_train_count')\n",
    "LastName_ParchSibSp_survived_counts = train_X_Y[train_X_Y[LABEL_COL] == 1].groupby(['LastName_ParchSibSp']).size().reset_index(name='LastName_ParchSibSp_train_survived_count')\n",
    "LastName_ParchSibSp_both_counts = LastName_ParchSibSp_counts.merge(LastName_ParchSibSp_survived_counts, on='LastName_ParchSibSp', how='left')\n",
    "LastName_ParchSibSp_both_counts['LastName_ParchSibSp_train_survived_count'].fillna(0, inplace=True)\n",
    "# Percent of each group who are known to survive in the train set\n",
    "# idea is that this will reflect a unseen group member's survival probability\n",
    "LastName_ParchSibSp_both_counts['train_surv_perc'] = LastName_ParchSibSp_both_counts['LastName_ParchSibSp_train_survived_count']/LastName_ParchSibSp_both_counts['LastName_ParchSibSp_train_count']\n",
    "train_and_test_X = train_and_test_X.merge(LastName_ParchSibSp_both_counts[['LastName_ParchSibSp', 'train_surv_perc']], how='left')\n",
    "\n",
    "# Subset to cols we'll use\n",
    "LABEL_COL = 'Survived'\n",
    "ID_COL = 'PassengerId'\n",
    "# These two lists control what raw features will be used, and how they will be treated\n",
    "numeric_cols = ['Fare', 'Age', 'CabinCount', 'ParchSibSp', 'Pclass']\n",
    "categorical_cols = ['Sex', 'Prefix', 'Embarked', 'CabinCode', 'TicketCode', 'train_surv_perc']\n",
    "\n",
    "# Subset to cols we'll use as features\n",
    "train_and_test_X = train_and_test_X[numeric_cols + categorical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Impute a particular column grouping by another column\n",
    "# Fill in with median if numeric, most common value otherwise\n",
    "#\n",
    "class ContextualImputer(TransformerMixin):\n",
    "    \n",
    "    # constructor params:\n",
    "    #  col : col to impute\n",
    "    #  byCol : column within which to impute 'col'\n",
    "    def __init__(self, col, byCol):\n",
    "        self.col = col\n",
    "        self.byCol = byCol\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.bycolumn_unique_values = np.unique(X[self.byCol].values)\n",
    "        \n",
    "        if X[self.col].dtype != np.dtype('O'):\n",
    "            bycolumn_fill_values = [np.median((X[pd.notnull(X[self.col]) & (X[self.byCol] == bycolumn_unique_value2)][self.col])) \n",
    "                                  for bycolumn_unique_value2 in self.bycolumn_unique_values]\n",
    "        else:\n",
    "            bycolumn_fill_values = [X[pd.notnull(X[self.col]) & (X[self.byCol] == bycolumn_unique_value)][self.col].value_counts().index[0]  \n",
    "                                  for bycolumn_unique_value in self.bycolumn_unique_values]\n",
    "\n",
    "        bycolumn_values_fill_zipped = np.column_stack((self.bycolumn_unique_values, bycolumn_fill_values))\n",
    "        self.bycolumn_values_fill_zipped_dict = dict(bycolumn_values_fill_zipped)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def fill(row):\n",
    "            if pd.isnull(row[self.col]):\n",
    "                return self.bycolumn_values_fill_zipped_dict[row[self.byCol]]\n",
    "            else:\n",
    "                return row[self.col]\n",
    "\n",
    "        X[self.col] = X[self.col].fillna(X.apply(fill, axis=1))\n",
    "        return X\n",
    "\n",
    "#\n",
    "# Typical imputer found around the web.  Most common value for categoricals,\n",
    "# median for numerics.  \n",
    "#\n",
    "# We'll use it for categoricals, and above customer imputer for numerics\n",
    "#\n",
    "class CategImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        If the Series is of dtype Object, then impute with the most frequent object.\n",
    "        If the Series is not of dtype Object, then impute with the mean.  \n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "                               if X[c].dtype == np.dtype('O')\n",
    "                               else X[c].median() for c in X], index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "#\n",
    "# Imputation of missing vals\n",
    "#\n",
    "all_X_imputed = ContextualImputer('Age', 'Pclass').fit_transform(train_and_test_X)\n",
    "all_X_imputed = ContextualImputer('Fare', 'Pclass').fit_transform(all_X_imputed)\n",
    "\n",
    "all_X_imputed['Embarked'] = np.where(all_X_imputed['Embarked'].isnull(), 'C', all_X_imputed['Embarked'])\n",
    "\n",
    "all_X_imputed['train_surv_perc'].fillna(PERC_SURVIVED, inplace=True)\n",
    "\n",
    "# most common value for categoricals and medians for numerics\n",
    "all_X_imputed = CategImputer().fit_transform(all_X_imputed)\n",
    "\n",
    "# all_X_imputed[all_X_imputed.isnull().any(axis=1)].shape # double check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X_encoded = all_X_imputed\n",
    "\n",
    "# Onehot encoding of cateogricals using get_dummies\n",
    "# Although Pclass is a number, it's really a code and we'll treat it as such\n",
    "for categorical_col in ['Pclass', 'Embarked']:\n",
    "    all_X_encoded = pd.concat([pd.get_dummies(all_X_imputed[categorical_col], \n",
    "                                              prefix=categorical_col, drop_first=True), \n",
    "                               all_X_encoded], axis=1)\n",
    "\n",
    "# can't leave categoricals numbers, Sex is binary so just map to 0/1\n",
    "all_X_encoded['Sex'] = all_X_encoded['Sex'].map({'male':0, 'female': 1})\n",
    "\n",
    "# Bin some features by survival likelihood, leave out last categ\n",
    "all_X_encoded[\"ParchSibSp_1\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if s == 1 else 0)\n",
    "all_X_encoded[\"ParchSibSp_23\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if (s == 2 or s == 3) else 0)\n",
    "all_X_encoded[\"ParchSibSp_4\"] = all_X_encoded[\"ParchSibSp\"].map(lambda s: 1 if (s == 4) else 0)\n",
    "\n",
    "# high_prob = ['Master.', Mme.', 'Ms.', 'Lady.', 'Sir.', 'Mlle.', 'Countess']\n",
    "# medhigh_prob = ['Mrs.', 'Miss.']\n",
    "# med_prob = ['Dr.', 'Major.', 'Col.', 'Mrs.']\n",
    "# low_prob = ['Mr.']\n",
    "# very_low_prob = ['Capt.', 'Don.', 'Rev.', 'Jonkheer.']\n",
    "# all_X_encoded[\"Prefix_high\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in high_prob else 0)\n",
    "# all_X_encoded[\"Prefix_medhigh_prob\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in medhigh_prob else 0)\n",
    "# all_X_encoded[\"Prefix_med\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in med_prob else 0)\n",
    "# all_X_encoded[\"Prefix_low\"] = all_X_encoded[\"Prefix\"].map(lambda s: 1 if s in low_prob else 0)\n",
    "\n",
    "# CabinCode\n",
    "# high_prob = ['E', 'D', 'B']\n",
    "# medhigh_prob = ['A', 'F']\n",
    "# med_prob = ['C', 'G']\n",
    "# low_prob = ['XX']\n",
    "# very_low_prob = ['T']\n",
    "# all_X_encoded[\"CabinCode_high\"] = all_X_encoded[\"CabinCode\"].map(lambda x: 1 if x in high_prob else 0)\n",
    "# all_X_encoded[\"CabinCode_medhigh\"] = all_X_encoded[\"CabinCode\"].map(lambda x: 1 if x in medhigh_prob else 0)\n",
    "# all_X_encoded[\"CabinCode_med\"] = all_X_encoded[\"CabinCode\"].map(lambda x: 1 if x in med_prob else 0)\n",
    "# all_X_encoded[\"CabinCode_low\"] = all_X_encoded[\"CabinCode\"].map(lambda x: 1 if x in low_prob else 0)\n",
    "\n",
    "# CabinCount\n",
    "# high_prob = [2, 3, 4]\n",
    "# med_prob = [1]\n",
    "# low_prob = [0]\n",
    "# all_X_encoded[\"CabinCount_high\"] = all_X_encoded[\"CabinCount\"].map(lambda x: 1 if x in high_prob else 0)\n",
    "# all_X_encoded[\"CabinCount_med\"] = all_X_encoded[\"CabinCount\"].map(lambda x: 1 if x in med_prob else 0)\n",
    "\n",
    "# Age bins\n",
    "# all_X_encoded[\"Age_child\"] = all_X_encoded[\"Age\"].map(lambda x: 1 if x < 15 else 0)\n",
    "# all_X_encoded[\"Age_youngadult\"] = all_X_encoded[\"Age\"].map(lambda x: 1 if (x >= 15 and x < 33) else 0)\n",
    "# all_X_encoded[\"Age_adult\"] = all_X_encoded[\"Age\"].map(lambda x: 1 if (x >= 33 and x < 55) else 0)\n",
    "\n",
    "# TicketCode\n",
    "high_prob = ['swpp', 'sc']\n",
    "med_high_prob = ['pc', 'pp', 'fcc', 'scah']\n",
    "med_prob = ['stono2', 'XX', 'ca', 'scparis', 'stono', 'c', 'ppp']\n",
    "low_prob = ['a5', 'soc', 'wc', 'sotonoq', 'wep']\n",
    "very_low_prob = ['sca4', 'a4', 'sp', 'spo', 'fa', 'scow', 'as', 'sopp', 'fc', 'sotono2', 'casoton']\n",
    "all_X_encoded[\"TicketCode_high\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in high_prob else 0)\n",
    "all_X_encoded[\"TicketCode_medhigh\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in med_high_prob else 0)\n",
    "all_X_encoded[\"TicketCode_med\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in med_prob else 0)\n",
    "all_X_encoded[\"TicketCode_low\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in low_prob else 0)\n",
    "all_X_encoded[\"TicketCode_very_low\"] = all_X_encoded[\"TicketCode\"].map(lambda s: 1 if s in very_low_prob else 0)\n",
    "\n",
    "all_X_final = all_X_encoded\n",
    "\n",
    "# Final dropping of features not to be used in classification\n",
    "all_X_final.drop(\"Prefix\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"TicketCode\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"CabinCode\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"CabinCount\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"Embarked\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"Pclass\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"Age\", inplace=True, axis=1)\n",
    "all_X_final.drop(\"train_surv_perc\", inplace=True, axis=1)\n",
    "\n",
    "train_X_matrix = all_X_final[0:train_df.shape[0]].as_matrix()\n",
    "train_Y_matrix = train_df[LABEL_COL]\n",
    "test_X = all_X_final[all_X_final.shape[0]-test_df.shape[0]::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homemade NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, \n",
    "                 learningrate, \n",
    "                 activation_function, inverse_activation_function):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.lr = learningrate\n",
    "        self.wih = (np.random.normal(0.0, pow(self.hnodes, -.5), (self.hnodes, self.inodes))) # array of shape hnodes by inodes\n",
    "        self.who = (np.random.normal(0.0, pow(self.onodes, -.5), (self.onodes, self.hnodes))) # array of shape hnodes by inodes\n",
    "        self.activation_function = activation_function\n",
    "        self.inverse_activation_function = inverse_activation_function\n",
    "        \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_ouputs = self.activation_function(final_inputs)\n",
    "\n",
    "        output_errors = targets - final_ouputs\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        self.who += self.lr * np.dot((output_errors * final_ouputs * (1.0 - final_ouputs)), np.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "\n",
    "    def query(self, inputs_list):\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_ouputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_ouputs\n",
    "    \n",
    "    #\n",
    "    # Wrapper for query which uses argmax to convert return values to hard labels (0,1)\n",
    "    #\n",
    "    def predict(self, inputs):\n",
    "        final_outputs = self.query(inputs)\n",
    "        return list(map(np.argmax, list(zip(final_outputs[0], final_outputs[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train NN across parameter combos, return best combo based on avg k-fold metric\n",
    "#\n",
    "# returns - (best params (dict), best model (neuralNetwork))\n",
    "# \n",
    "def cross_train_neural_net_cv(train_X_scaled, train_Y,  \n",
    "                              hidden_node_count_multiplier_params, learning_rate_params, epoch_params, \n",
    "                              activation_functions,\n",
    "                              n_folds=2, metric=accuracy_score, metric_function=np.mean,\n",
    "                              verbose=0):\n",
    "    \n",
    "    num_input_nodes = train_X_scaled.shape[1]\n",
    "    num_output_nodes = 2\n",
    "    \n",
    "    # number of nodes in the hidden layer is average of input and output \n",
    "    # multiplied by hidden_node_count_multiplier_params\n",
    "    avg_nodes_input_output = int((float(num_input_nodes) + float(num_output_nodes))/2)\n",
    "    num_hidden_nodes = list(map(lambda x: int(round(x[0] * x[1])), zip(hidden_node_count_multiplier_params, \n",
    "                                                                   np.full(len(hidden_node_count_multiplier_params), \n",
    "                                                                           avg_nodes_input_output))))\n",
    "\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    num_combos = len(learning_rate_params) * len(num_hidden_nodes) * len(epoch_params)\n",
    "    combo_count = 0\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = n_folds)\n",
    "    \n",
    "    for learning_rate in learning_rate_params:\n",
    "        for hidden_node_count in num_hidden_nodes:\n",
    "            for epochs in epoch_params:\n",
    "                for activation_function_tuple in activation_functions:\n",
    "\n",
    "                    skf_indices = skf.split(train_X_scaled, train_Y)\n",
    "                    combo_count += 1\n",
    "                    if (verbose > 0) and ((combo_count % 20) == 0) :\n",
    "                        print(\"cross_train_neural_net_holdout : processed {0} of {1} parameter combinations.\".format(combo_count, num_combos))\n",
    "\n",
    "                    fold_scores = []\n",
    "                    for train_index, test_index in skf_indices:\n",
    "\n",
    "                        train_fold_X = train_X_scaled[train_index]\n",
    "                        train_fold_Y = train_Y[train_index]\n",
    "                        test_fold_X = train_X_scaled[test_index]\n",
    "                        test_fold_Y = train_Y[test_index]\n",
    "\n",
    "                        trained_nn = train_nn_cv(num_input_nodes, num_output_nodes, \n",
    "                                                 hidden_node_count, learning_rate, epochs, \n",
    "                                                 activation_function_tuple[0], activation_function_tuple[1], \n",
    "                                                 train_fold_X, train_fold_Y)\n",
    "                        \n",
    "                        fold_scores.append(calculate_score(trained_nn, \n",
    "                                                             learning_rate, epochs, hidden_node_count, \n",
    "                                                             test_fold_X, test_fold_Y,\n",
    "                                                             metric=metric, verbose=verbose))\n",
    "\n",
    "                    param_combo_score = metric_function(fold_scores)\n",
    "                    \n",
    "                    if verbose > 1:\n",
    "                        print(\"fold_scores = {0}\".format(fold_scores))\n",
    "\n",
    "                    if param_combo_score > best_score:\n",
    "                        best_score = param_combo_score\n",
    "                        best_params = {\"hidden_node_count\": hidden_node_count, \n",
    "                                       \"learning_rate\": learning_rate, \n",
    "                                       \"epochs\": epochs,\n",
    "                                       \"activation_functions\": activation_function_tuple\n",
    "                                      }\n",
    "                \n",
    "        # finally train all data on best param combo\n",
    "        best_nn = train_nn_cv(num_input_nodes, num_output_nodes, \n",
    "                           best_params['hidden_node_count'], best_params['learning_rate'], best_params['epochs'], \n",
    "                           best_params['activation_functions'][0], best_params['activation_functions'][1],\n",
    "                           train_X_scaled, train_Y)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"\\nBEST SCORE : {0}\\nBEST_PARAMS : {1}\".format(best_score, best_params))\n",
    "\n",
    "    return (best_params, best_nn)\n",
    "\n",
    "#\n",
    "# Train neural net with given params\n",
    "#\n",
    "def train_nn_cv(num_input_nodes, num_output_nodes, \n",
    "                hidden_node_count, lr, epochs, \n",
    "                activation_function, inverse_activation_function,\n",
    "                train_X_scaled, train_Y):\n",
    "    \n",
    "    trained_nn = neuralNetwork(num_input_nodes, hidden_node_count, num_output_nodes, \n",
    "                               lr, \n",
    "                               activation_function, inverse_activation_function)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        rowcount = 0\n",
    "        train_combined = np.column_stack((train_X_scaled, train_Y))\n",
    "        \n",
    "        # train on each row\n",
    "        for row in train_combined:\n",
    "            targets = np.zeros(num_output_nodes) + 0.01\n",
    "            targets[int(row[train_combined.shape[1]-1])] = .99\n",
    "            trained_nn.train(row[0:train_combined.shape[1]-1], targets)   \n",
    "            rowcount += 1\n",
    "            \n",
    "    return trained_nn\n",
    "\n",
    "#\n",
    "# Test a neural network for a particular param combo and metric type and return the score\n",
    "# \n",
    "# args:\n",
    "#   nn - trained neural net classifier\n",
    "#   test_data_X - data to predict labels for and compare to actual\n",
    "#   test_data_Y - actual labels\n",
    "#   lr, epochs, hidden_node_count = params for nn\n",
    "#\n",
    "def calculate_score(nn, lr, epochs, hidden_node_count, \n",
    "                    test_data_X, test_data_Y, metric=accuracy_score, verbose=0):\n",
    "    \n",
    "    labels = nn.predict(test_data_X)\n",
    "    \n",
    "    try:\n",
    "        score = metric(labels, test_data_Y)\n",
    "\n",
    "        if verbose > 1 :\n",
    "            print(\"calculate_score({4}) : epochs : {0}, lr : {1}, hidden_node_count : {2}, test score = {3}\".format(epochs, lr, \n",
    "                                                                                                                hidden_node_count, \n",
    "                                                                                                                score, str(metric)))\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred scoring : {0}\".format(e))\n",
    "        print(\"  epochs : {0}, lr : {1}, hidden_node_count : {2}, metric : {3}\".format(epochs, \n",
    "                                                                                       lr, \n",
    "                                                                                       hidden_node_count, \n",
    "                                                                                       str(metric)))\n",
    "        return 0\n",
    "\n",
    "#\n",
    "# Return the lower bound of the 99 percent confidence interval of the given list of scores\n",
    "# Used as a comparison function in cross validation\n",
    "#\n",
    "def conf_int_low_bound(scores):\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    return mean - 3*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_train_neural_net_holdout : processed 20 of 72 parameter combinations.\n",
      "cross_train_neural_net_holdout : processed 40 of 72 parameter combinations.\n",
      "cross_train_neural_net_holdout : processed 60 of 72 parameter combinations.\n",
      "\n",
      "BEST SCORE : 0.7617161981327182\n",
      "BEST_PARAMS : {'hidden_node_count': 64, 'learning_rate': 0.01, 'epochs': 10, 'activation_functions': (<ufunc 'expit'>, <ufunc 'logit'>)}\n",
      "NN : num inputs : 15\n",
      "NN : CV elapsed time : 390.24461007118225\n",
      "Last run time : 2018-04-17 08:26:08.820887\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Fit scaler from ALL data\n",
    "scaler = MinMaxScaler().fit(all_X_final)\n",
    "\n",
    "# this will be multiplied by the average of the number of input and output nodes\n",
    "#HIDDEN_NODE_COUNT_MULTIPLIERS = [1.0]\n",
    "HIDDEN_NODE_COUNT_MULTIPLIERS = [.5, 1.0, 1.5, 2.0, 4.0, 8.0]\n",
    "#LEARNING_RATES = [.01]\n",
    "LEARNING_RATES = [.001, .005, .01, .1]\n",
    "#EPOCHS = [10]\n",
    "EPOCHS = [10, 20, 30]\n",
    "\n",
    "# pairs of activation/inverse functions\n",
    "ACTIVATION_FUNCTIONS = [(scipy.special.expit, scipy.special.logit)]\n",
    "\n",
    "# Train and pick best model based on CV metric\n",
    "train_X_scaled = scaler.transform(train_X_matrix)\n",
    "best_params, best_nn = cross_train_neural_net_cv(train_X_scaled, train_Y_matrix, \n",
    "                                                 HIDDEN_NODE_COUNT_MULTIPLIERS, LEARNING_RATES, EPOCHS, ACTIVATION_FUNCTIONS, \n",
    "                                                 n_folds=10, metric=accuracy_score, metric_function=conf_int_low_bound,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# apply best_nn to test data and save results\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "test_predictions = best_nn.predict(test_X_scaled)\n",
    "nn_final_predictions_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': test_predictions})\n",
    "nn_final_predictions_df.to_csv(\"submission_nn.csv\", index=False)\n",
    "\n",
    "print(\"NN : num inputs : {0}\".format(best_nn.inodes))\n",
    "print(\"NN : CV elapsed time : {0}\".format((time.time() - start_time)))\n",
    "print(\"Last run time : {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Fare',\n",
       "       'ParchSibSp', 'Sex', 'ParchSibSp_1', 'ParchSibSp_23', 'ParchSibSp_4',\n",
       "       'TicketCode_high', 'TicketCode_medhigh', 'TicketCode_med',\n",
       "       'TicketCode_low', 'TicketCode_very_low'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7],\n",
       "       [78, 79, 80],\n",
       "       [ 2,  3,  4],\n",
       "       [34, 35, 36],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odd = np.array([[1, 3], [5, 7]])\n",
    "even = np.array([[2, 4], [6, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [5, 7]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [6, 8]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 12],\n",
       "       [30, 56]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd * even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 28],\n",
       "       [52, 76]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(odd, even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
